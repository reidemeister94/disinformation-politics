{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from dateutil import parser\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import unidecode\n",
    "import emoji\n",
    "import json\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sp = spacy.load(\"it_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_PAGE_LIST = [\"politicians\"]\n",
    "TYPE_SOCIAL_LIST = [\"facebook\"]  # [\"facebook\", \"instagram\"]\n",
    "COUNTRY_PAGE_LIST = [\"italy\"]\n",
    "\n",
    "DATA_PATH = \"Data/\"\n",
    "\n",
    "COLUMNS_TYPES_FB = {}\n",
    "COLUMNS_TYPES_IG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians_party_map = {\n",
    "    \"Matteo Salvini\": \"Lega - Salvini Premier\",\n",
    "    \"Luigi Di Maio\": \"MoVimento 5 Stelle\",\n",
    "    \"Silvio Berlusconi\": \"Forza Italia\",\n",
    "    \"Nicola Zingaretti\": \"Partito Democratico\",\n",
    "    \"Emma Bonino\": \"PiÃ¹ Europa\",\n",
    "    \"Giorgia Meloni\": \"Fratelli d'Italia\",\n",
    "    \"Matteo Renzi\": \"Italia Viva\",\n",
    "    \"Nicola Fratoianni\": \"\",\n",
    "    \"Giuseppe Conte\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHS_2019 = {\"dec\": 12}\n",
    "MONTHS_2020 = {\n",
    "    \"jan\": 1,\n",
    "    \"feb\": 2,\n",
    "    \"mar\": 3,\n",
    "    \"apr\": 4,\n",
    "    \"may\": 5,\n",
    "    \"jun\": 6,\n",
    "    \"jul\": 7,\n",
    "    \"aug\": 8,\n",
    "}\n",
    "df_map = {}\n",
    "df_months_map = {\n",
    "    \"dec\": {},\n",
    "    \"jan\": {},\n",
    "    \"feb\": {},\n",
    "    \"mar\": {},\n",
    "    \"apr\": {},\n",
    "    \"may\": {},\n",
    "    \"jun\": {},\n",
    "    \"jul\": {},\n",
    "    \"aug\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silvio/opt/anaconda3/envs/ricerca/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (29) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/silvio/opt/anaconda3/envs/ricerca/lib/python3.7/site-packages/dateutil/parser/_parser.py:1218: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n",
      "/Users/silvio/opt/anaconda3/envs/ricerca/lib/python3.7/site-packages/dateutil/parser/_parser.py:1218: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    }
   ],
   "source": [
    "for type_page in TYPE_PAGE_LIST:\n",
    "    for type_social in TYPE_SOCIAL_LIST:\n",
    "        for country_page in COUNTRY_PAGE_LIST:\n",
    "            name_df = \"_\".join((type_page, type_social, country_page))\n",
    "            df_map[name_df] = pd.read_csv(DATA_PATH + name_df + \".csv\", header=0,)\n",
    "            df_map[name_df][\"Created\"] = df_map[name_df][\"Created\"].apply(\n",
    "                lambda x: parser.parse(x)\n",
    "            )\n",
    "            if type_social == \"instagram\":\n",
    "                df_map[name_df][\"Description\"] = df_map[name_df][\"Description\"].fillna(\n",
    "                    value=\"\"\n",
    "                )\n",
    "            else:\n",
    "                df_map[name_df][\"Message\"] = df_map[name_df][\"Message\"].fillna(value=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_map[\"politicians_facebook_italy\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DFs Map per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_df, df in df_map.items():\n",
    "    df_months_map[list(MONTHS_2019.keys())[0]][name_df] = df_map[name_df][\n",
    "        (df_map[name_df][\"Created\"] >= \"2019-12-01\")\n",
    "        & (df_map[name_df][\"Created\"] < \"2020-01-01\")\n",
    "    ].copy()\n",
    "\n",
    "for name_df, df in df_map.items():\n",
    "    for month in MONTHS_2020.keys():\n",
    "        df_months_map[month][name_df] = df_map[name_df][\n",
    "            (df_map[name_df][\"Created\"] >= \"2020-{}-01\".format(MONTHS_2020[month]))\n",
    "            & (df_map[name_df][\"Created\"] < \"2020-{}-01\".format(MONTHS_2020[month] + 1))\n",
    "        ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2754"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_map\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Text Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = sp.Defaults.stop_words\n",
    "with open(\"stopwords_italian.json\") as json_file:\n",
    "    italian_stopwords = json.load(json_file)\n",
    "all_stopwords |= set(italian_stopwords[\"stopwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(u\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grey_color_func(\n",
    "    word, font_size, position, orientation, random_state=None, **kwargs\n",
    "):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_text_blocks = {\n",
    "    \"dec\": defaultdict(str),\n",
    "    \"jan\": defaultdict(str),\n",
    "    \"feb\": defaultdict(str),\n",
    "    \"mar\": defaultdict(str),\n",
    "    \"apr\": defaultdict(str),\n",
    "    \"may\": defaultdict(str),\n",
    "    \"jun\": defaultdict(str),\n",
    "    \"jul\": defaultdict(str),\n",
    "    \"aug\": defaultdict(str),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = remove_emoji(text)\n",
    "    # text = unidecode.unidecode(text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"#\\S+:\", \"\", text)\n",
    "    text = re.sub(\"# \\S+ :\", \"\", text)\n",
    "    text = re.sub(\"#\\S+ :\", \"\", text)\n",
    "    text = re.sub(\"# \\S+:\", \"\", text)\n",
    "    text = re.sub(\"#\\S+\", \"\", text)\n",
    "    text = re.sub(\"legaonline.it\\S+\", \"\", text)\n",
    "    text = re.sub(\"[,\\.!?#]\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = (\n",
    "        text.replace(\"http\", \"\")\n",
    "        .replace(\"www\", \"\")\n",
    "        .replace(\"shortener\", \"\")\n",
    "        .replace(\"ref\", \"\")\n",
    "        .replace(\"matteo salvini\", \"salvini\")\n",
    "        .replace(\"user\", \"\")\n",
    "        .replace(\"legaonline.it/iostoconsalvini\", \"\")\n",
    "    )\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    # stop_words = set(stopwords.words(\"italian\"))\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "    res = \" \".join(tokens_without_sw)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month, df_map_month in df_months_map.items():\n",
    "    for df_name, df_data in df_map_month.items():\n",
    "        for pol, party in politicians_party_map.items():\n",
    "            complete_name = (\n",
    "                pol.lower().replace(\" - \", \" \").replace(\" \", \"_\")\n",
    "                + \"__\"\n",
    "                + party.lower().replace(\" - \", \" \").replace(\" \", \"_\")\n",
    "            )\n",
    "            if len(party) > 0:\n",
    "                df_specific_party = df_data.loc[\n",
    "                    df_data[\"Page Name\"].isin([pol, party])\n",
    "                ].copy()\n",
    "            else:\n",
    "                df_specific_party = df_data.loc[df_data[\"Page Name\"].isin([pol])].copy()\n",
    "            df_specific_party[\"Message\"] = df_specific_party[\"Message\"].apply(\n",
    "                lambda x: clean_text(x)\n",
    "            )\n",
    "            month_text_blocks[month][complete_name] += \" \".join(\n",
    "                list(df_specific_party[\"Message\"].values)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month_text_blocks[\"aug\"][\"luigi_di_maio__movimento_5_stelle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phases_text_blocks = {\n",
    "#   phase: clean_text(text) for phase, text in phases_text_blocks.items()\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"temp.txt\", \"w\") as writer:\n",
    "#    writer.write(phases_text_blocks[\"post_covid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and generate wordclouds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month, map_texts in month_text_blocks.items():\n",
    "    for pol_party, text_data in map_texts.items():\n",
    "        wordcloud = WordCloud(\n",
    "            max_font_size=50,\n",
    "            max_words=50,\n",
    "            # background_color=\"darkblue\",\n",
    "            # colormap=\"Blues\",\n",
    "            min_font_size=10,\n",
    "            margin=10,\n",
    "            random_state=1,\n",
    "            width=750,\n",
    "            height=500,\n",
    "        ).generate(text_data)\n",
    "        default_colors = wordcloud.to_array()\n",
    "        plt.figure()\n",
    "        plt.title(\n",
    "            pol_party.replace(\"_\", \" \").replace(\"__\", \" \").capitalize(), fontsize=20\n",
    "        )\n",
    "        plt.imshow(\n",
    "            wordcloud.recolor(color_func=grey_color_func, random_state=3),\n",
    "            interpolation=\"bilinear\",\n",
    "        )\n",
    "        # plt.imshow(wordcloud, interpolation=\"bilinear\", aspect=\"auto\")\n",
    "        plt.axis(\"off\")\n",
    "        if not path.isdir(\"Output/lda_wordcloud_by_month/wordclouds/\" + month):\n",
    "            os.mkdir(\"Output/lda_wordcloud_by_month/wordclouds/\" + month)\n",
    "        plt.savefig(\n",
    "            \"Output/lda_wordcloud_by_month/wordclouds/\"\n",
    "            + month\n",
    "            + \"/\"\n",
    "            + pol_party\n",
    "            + \".png\",\n",
    "            dpi=300,\n",
    "        )\n",
    "        plt.close(\"all\")\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the library with the CountVectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer, phase, name):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        # print(t)\n",
    "        total_counts += t.toarray()[0]\n",
    "\n",
    "    count_dict = zip(words, total_counts)\n",
    "    count_dict = sorted(count_dict, key=lambda x: x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words))\n",
    "\n",
    "    plt.figure(2, figsize=(15, 15 / 1.6180))\n",
    "    plt.subplot(\n",
    "        title=\"10 most common words - {} - {}\".format(\n",
    "            phase.replace(\"_\", \" \").capitalize(), name.replace(\"_\", \" \").capitalize()\n",
    "        )\n",
    "    )\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette=\"husl\")\n",
    "    plt.xticks(x_pos, words, rotation=90)\n",
    "    plt.xlabel(\"words\")\n",
    "    plt.ylabel(\"counts\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data_map = {\"pre_covid\": {}, \"during_covid\": {}, \"post_covid\": {}}\n",
    "count_vectorizer_map = {\"pre_covid\": {}, \"during_covid\": {}, \"post_covid\": {}}\n",
    "lda_map = {\"pre_covid\": {}, \"during_covid\": {}, \"post_covid\": {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Immigration words to avoid redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_immigration = [\n",
    "    \"immigrati\",\n",
    "    \"immigrate\",\n",
    "    \"immigrato\",\n",
    "    \"migranti\",\n",
    "    \"migrante\",\n",
    "    \"immigrazione\",\n",
    "    \"sbarchi\",\n",
    "    \"sbarco\",\n",
    "    \"clandestini\",\n",
    "    \"sbarcati\",\n",
    "    \"sbarcato\",\n",
    "    \"clandestino\",\n",
    "    \"barconi\",\n",
    "]\n",
    "\n",
    "\n",
    "def clean_text_immigration(text):\n",
    "    for w in words_immigration:\n",
    "        text = text.replace(w, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase, dicts in df_covid_phase_map.items():\n",
    "    for name, df in dicts.items():\n",
    "        if name.split(\"_\")[0] == \"politicians\":\n",
    "            if name.split(\"_\")[1] == \"instagram\":\n",
    "                df[\"Description\"] = df[\"Description\"].apply(\n",
    "                    lambda x: clean_text_immigration(x)\n",
    "                )\n",
    "            else:\n",
    "                # facebook\n",
    "                df[\"Message\"] = df[\"Message\"].apply(lambda x: clean_text_immigration(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the count vectorizer with the Italian stop words - Fit and transform the processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase, dicts in df_covid_phase_map.items():\n",
    "    for name, df in dicts.items():\n",
    "        if name.split(\"_\")[0] == \"politicians\":\n",
    "            count_vectorizer = CountVectorizer(stop_words=all_stopwords)\n",
    "            if name.split(\"_\")[1] == \"instagram\":\n",
    "                vectorized_data_map[phase][name] = count_vectorizer.fit_transform(\n",
    "                    df[\"Description\"]\n",
    "                )\n",
    "            else:\n",
    "                vectorized_data_map[phase][name] = count_vectorizer.fit_transform(\n",
    "                    df[\"Message\"]\n",
    "                )\n",
    "            count_vectorizer_map[phase][name] = count_vectorizer\n",
    "            del count_vectorizer\n",
    "            gc.collect()\n",
    "            plot_10_most_common_words(\n",
    "                vectorized_data_map[phase][name],\n",
    "                count_vectorizer_map[phase][name],\n",
    "                phase,\n",
    "                name,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and fit the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "\n",
    "# warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(len(topic))\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[: -n_top_words - 1 : -1]]))\n",
    "\n",
    "\n",
    "# Tweak the two parameters below\n",
    "number_topics = 8\n",
    "number_words = 6\n",
    "\n",
    "for phase, dicts in vectorized_data_map.items():\n",
    "    for name, df in dicts.items():\n",
    "        lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "        lda.fit_transform(df)\n",
    "        lda_map[phase][name] = lda\n",
    "        del lda\n",
    "        gc.collect()\n",
    "        # Print the topics found by the LDA model\n",
    "        # print(\"Topics found via LDA:\")\n",
    "        # print_topics(\n",
    "        #    lda_map[phase][name], count_vectorizer_map[phase][name], number_words\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "\n",
    "for phase, dicts in vectorized_data_map.items():\n",
    "    for name, df in dicts.items():\n",
    "        if name.split(\"_\")[0] == \"politicians\":\n",
    "            LDAvis_data_filepath = 'Output/lda/ldavis_{}_{}_{}_{}.pickle'.format(phase,name,number_topics, number_words)\n",
    "            LDAvis_prepared = sklearn_lda.prepare(lda_map[phase][name], df, count_vectorizer_map[phase][name])\n",
    "            with open(LDAvis_data_filepath, 'wb') as f:\n",
    "                pickle.dump(LDAvis_prepared, f)\n",
    "            pyLDAvis.save_html(LDAvis_prepared,'Output/lda/ldavis_{}_{}_{}_{}.html'.format(phase,name,number_topics, number_words))\n",
    "    \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "#with open(LDAvis_data_filepath) as f:\n",
    "#    LDAvis_prepared = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ricerca",
   "language": "python",
   "name": "ricerca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
