{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from dateutil import parser\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import unidecode\n",
    "import emoji\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sp = spacy.load(\"it_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_PAGE_LIST = [\"politicians\"]\n",
    "TYPE_SOCIAL_LIST = [\"facebook\"]  # [\"facebook\", \"instagram\"]\n",
    "COUNTRY_PAGE_LIST = [\"italy\"]\n",
    "\n",
    "DATA_PATH = \"Data/\"\n",
    "\n",
    "COLUMNS_TYPES_FB = {}\n",
    "COLUMNS_TYPES_IG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians_party_map = {\n",
    "    \"Matteo Salvini\": \"Lega - Salvini Premier\",\n",
    "    \"Luigi Di Maio\": \"MoVimento 5 Stelle\",\n",
    "    \"Silvio Berlusconi\": \"Forza Italia\",\n",
    "    \"Nicola Zingaretti\": \"Partito Democratico\",\n",
    "    \"Emma Bonino\": \"PiÃ¹ Europa\",\n",
    "    \"Giorgia Meloni\": \"Fratelli d'Italia\",\n",
    "    \"Matteo Renzi\": \"Italia Viva\",\n",
    "    \"Nicola Fratoianni\": \"\",\n",
    "    \"Giuseppe Conte\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHS_2019 = {\"dec\": 12}\n",
    "MONTHS_2020 = {\n",
    "    \"jan\": 1,\n",
    "    \"feb\": 2,\n",
    "    \"mar\": 3,\n",
    "    \"apr\": 4,\n",
    "    \"may\": 5,\n",
    "    \"jun\": 6,\n",
    "    \"jul\": 7,\n",
    "    \"aug\": 8,\n",
    "}\n",
    "df_map = {}\n",
    "df_months_map = {\n",
    "    \"dec\": {},\n",
    "    \"jan\": {},\n",
    "    \"feb\": {},\n",
    "    \"mar\": {},\n",
    "    \"apr\": {},\n",
    "    \"may\": {},\n",
    "    \"jun\": {},\n",
    "    \"jul\": {},\n",
    "    \"aug\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_page in TYPE_PAGE_LIST:\n",
    "    for type_social in TYPE_SOCIAL_LIST:\n",
    "        for country_page in COUNTRY_PAGE_LIST:\n",
    "            name_df = \"_\".join((type_page, type_social, country_page))\n",
    "            df_map[name_df] = pd.read_csv(DATA_PATH + name_df + \".csv\", header=0,)\n",
    "            df_map[name_df][\"Created\"] = df_map[name_df][\"Created\"].apply(\n",
    "                lambda x: parser.parse(x)\n",
    "            )\n",
    "            if type_social == \"instagram\":\n",
    "                df_map[name_df][\"Description\"] = df_map[name_df][\"Description\"].fillna(\n",
    "                    value=\"\"\n",
    "                )\n",
    "            else:\n",
    "                df_map[name_df][\"Message\"] = df_map[name_df][\"Message\"].fillna(value=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_map[\"politicians_facebook_italy\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DFs Map per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_df, df in df_map.items():\n",
    "    df_months_map[list(MONTHS_2019.keys())[0]][name_df] = df_map[name_df][\n",
    "        (df_map[name_df][\"Created\"] >= \"2019-12-01\")\n",
    "        & (df_map[name_df][\"Created\"] < \"2020-01-01\")\n",
    "    ].copy()\n",
    "\n",
    "for name_df, df in df_map.items():\n",
    "    for month in MONTHS_2020.keys():\n",
    "        df_months_map[month][name_df] = df_map[name_df][\n",
    "            (df_map[name_df][\"Created\"] >= \"2020-{}-01\".format(MONTHS_2020[month]))\n",
    "            & (df_map[name_df][\"Created\"] < \"2020-{}-01\".format(MONTHS_2020[month] + 1))\n",
    "        ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_map\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Text Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = sp.Defaults.stop_words\n",
    "with open(\"stopwords_italian.json\") as json_file:\n",
    "    italian_stopwords = json.load(json_file)\n",
    "all_stopwords |= set(italian_stopwords[\"stopwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(u\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grey_color_func(\n",
    "    word, font_size, position, orientation, random_state=None, **kwargs\n",
    "):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_text_blocks = {\n",
    "    \"dec\": defaultdict(str),\n",
    "    \"jan\": defaultdict(str),\n",
    "    \"feb\": defaultdict(str),\n",
    "    \"mar\": defaultdict(str),\n",
    "    \"apr\": defaultdict(str),\n",
    "    \"may\": defaultdict(str),\n",
    "    \"jun\": defaultdict(str),\n",
    "    \"jul\": defaultdict(str),\n",
    "    \"aug\": defaultdict(str),\n",
    "}\n",
    "df_specific_party_map = {\n",
    "    \"dec\": {},\n",
    "    \"jan\": {},\n",
    "    \"feb\": {},\n",
    "    \"mar\": {},\n",
    "    \"apr\": {},\n",
    "    \"may\": {},\n",
    "    \"jun\": {},\n",
    "    \"jul\": {},\n",
    "    \"aug\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = remove_emoji(text)\n",
    "    # text = unidecode.unidecode(text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"#\\S+:\", \"\", text)\n",
    "    text = re.sub(\"# \\S+ :\", \"\", text)\n",
    "    text = re.sub(\"#\\S+ :\", \"\", text)\n",
    "    text = re.sub(\"# \\S+:\", \"\", text)\n",
    "    text = re.sub(\"#\\S+\", \"\", text)\n",
    "    text = re.sub(\"legaonline.it\\S+\", \"\", text)\n",
    "    text = re.sub(\"[,\\.!?#]\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = (\n",
    "        text.replace(\"http\", \"\")\n",
    "        .replace(\"www\", \"\")\n",
    "        .replace(\"shortener\", \"\")\n",
    "        .replace(\"ref\", \"\")\n",
    "        .replace(\"matteo salvini\", \"salvini\")\n",
    "        .replace(\"user\", \"\")\n",
    "        .replace(\"legaonline.it/iostoconsalvini\", \"\")\n",
    "    )\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    # stop_words = set(stopwords.words(\"italian\"))\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "    res = \" \".join(tokens_without_sw)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month, df_map_month in df_months_map.items():\n",
    "    for df_name, df_data in df_map_month.items():\n",
    "        for pol, party in politicians_party_map.items():\n",
    "            if len(party) > 0:\n",
    "                complete_name = (\n",
    "                    pol.lower().replace(\" - \", \" \").replace(\" \", \"_\")\n",
    "                    + \"__\"\n",
    "                    + party.lower().replace(\" - \", \" \").replace(\" \", \"_\")\n",
    "                )\n",
    "                df_specific_party_map[month][complete_name] = df_data.loc[\n",
    "                    df_data[\"Page Name\"].isin([pol, party])\n",
    "                ].copy()\n",
    "            else:\n",
    "                complete_name = pol.lower().replace(\" - \", \" \").replace(\" \", \"_\")\n",
    "                df_specific_party_map[month][complete_name] = df_data.loc[\n",
    "                    df_data[\"Page Name\"].isin([pol])\n",
    "                ].copy()\n",
    "            df_specific_party_map[month][complete_name][\n",
    "                \"Message\"\n",
    "            ] = df_specific_party_map[month][complete_name][\"Message\"].apply(\n",
    "                lambda x: clean_text(x)\n",
    "            )\n",
    "            month_text_blocks[month][complete_name] += \" \".join(\n",
    "                list(df_specific_party_map[month][complete_name][\"Message\"].values)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and generate wordclouds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month, map_texts in month_text_blocks.items():\n",
    "    for pol_party, text_data in map_texts.items():\n",
    "        wordcloud = WordCloud(\n",
    "            # max_font_size=50,\n",
    "            # max_words=50,\n",
    "            # background_color=\"darkblue\",\n",
    "            # colormap=\"Blues\",\n",
    "            # min_font_size=10,\n",
    "            # margin=10,\n",
    "            # random_state=1,\n",
    "            width=750,\n",
    "            height=500,\n",
    "        ).generate(text_data)\n",
    "        # default_colors = wordcloud.to_array()\n",
    "        plt.figure()\n",
    "        plt.title(\n",
    "            pol_party.replace(\"_\", \" \").replace(\"__\", \" \").capitalize(), fontsize=20\n",
    "        )\n",
    "        plt.imshow(\n",
    "            wordcloud,\n",
    "            # wordcloud.recolor(color_func=grey_color_func, random_state=3),\n",
    "            interpolation=\"bilinear\",\n",
    "        )\n",
    "        # plt.imshow(wordcloud, interpolation=\"bilinear\", aspect=\"auto\")\n",
    "        plt.axis(\"off\")\n",
    "        if not path.isdir(\"Output/lda_wordcloud_by_month/wordclouds/\" + month):\n",
    "            os.mkdir(\"Output/lda_wordcloud_by_month/wordclouds/\" + month)\n",
    "        plt.savefig(\n",
    "            \"Output/lda_wordcloud_by_month/wordclouds/\"\n",
    "            + month\n",
    "            + \"/\"\n",
    "            + pol_party\n",
    "            + \".png\",\n",
    "            dpi=300,\n",
    "        )\n",
    "        plt.close(\"all\")\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer, month, name):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        # print(t)\n",
    "        total_counts += t.toarray()[0]\n",
    "\n",
    "    count_dict = zip(words, total_counts)\n",
    "    count_dict = sorted(count_dict, key=lambda x: x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words))\n",
    "\n",
    "    plt.figure(2, figsize=(15, 15 / 1.6180))\n",
    "    plt.subplot(\n",
    "        title=\"10 most common words - {} - {}\".format(\n",
    "            month.replace(\"_\", \" \").capitalize(), name.replace(\"_\", \" \").capitalize()\n",
    "        )\n",
    "    )\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette=\"husl\")\n",
    "    plt.xticks(x_pos, words, rotation=90)\n",
    "    plt.xlabel(\"words\")\n",
    "    plt.ylabel(\"counts\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data_map = {\n",
    "    \"dec\": {},\n",
    "    \"jan\": {},\n",
    "    \"feb\": {},\n",
    "    \"mar\": {},\n",
    "    \"apr\": {},\n",
    "    \"may\": {},\n",
    "    \"jun\": {},\n",
    "    \"jul\": {},\n",
    "    \"aug\": {},\n",
    "}\n",
    "count_vectorizer_map = {\n",
    "    \"dec\": {},\n",
    "    \"jan\": {},\n",
    "    \"feb\": {},\n",
    "    \"mar\": {},\n",
    "    \"apr\": {},\n",
    "    \"may\": {},\n",
    "    \"jun\": {},\n",
    "    \"jul\": {},\n",
    "    \"aug\": {},\n",
    "}\n",
    "lda_map = {\n",
    "    \"dec\": {},\n",
    "    \"jan\": {},\n",
    "    \"feb\": {},\n",
    "    \"mar\": {},\n",
    "    \"apr\": {},\n",
    "    \"may\": {},\n",
    "    \"jun\": {},\n",
    "    \"jul\": {},\n",
    "    \"aug\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Immigration words to avoid redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_immigration = [\n",
    "    \"immigrati\",\n",
    "    \"immigrate\",\n",
    "    \"immigrato\",\n",
    "    \"migranti\",\n",
    "    \"migrante\",\n",
    "    \"immigrazione\",\n",
    "    \"sbarchi\",\n",
    "    \"sbarco\",\n",
    "    \"clandestini\",\n",
    "    \"sbarcati\",\n",
    "    \"sbarcato\",\n",
    "    \"clandestino\",\n",
    "    \"barconi\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_immigration(text):\n",
    "    for w in words_immigration:\n",
    "        text = text.replace(w, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month, dicts in df_specific_party_map.items():\n",
    "    for name, df in dicts.items():\n",
    "        # facebook\n",
    "        df[\"Message\"] = df[\"Message\"].apply(lambda x: clean_text_immigration(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the count vectorizer with the Italian stop words - Fit and transform the processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month, dicts in df_specific_party_map.items():\n",
    "    for name, df in dicts.items():\n",
    "        count_vectorizer = CountVectorizer(stop_words=all_stopwords)\n",
    "        vectorized_data_map[month][name] = count_vectorizer.fit_transform(df[\"Message\"])\n",
    "        count_vectorizer_map[month][name] = count_vectorizer\n",
    "        del count_vectorizer\n",
    "        gc.collect()\n",
    "        # plot_10_most_common_words(\n",
    "        #    vectorized_data_map[month][name],\n",
    "        #    count_vectorizer_map[month][name],\n",
    "        #    month,\n",
    "        #    name,\n",
    "        # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and fit the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(len(topic))\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[: -n_top_words - 1 : -1]]))\n",
    "\n",
    "\n",
    "# Tweak the two parameters below\n",
    "number_topics = 4\n",
    "number_words = 6\n",
    "\n",
    "print(\"Topics found via LDA:\")\n",
    "for month, dicts in vectorized_data_map.items():\n",
    "    print(\"â\" * 75)\n",
    "    print(\"â\" * 75)\n",
    "    print(\"MONTH: {}\".format(month))\n",
    "    print(\"â\" * 75)\n",
    "    for name, df in dicts.items():\n",
    "        print(\"NAME: {}\".format(name))\n",
    "        lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "        lda.fit_transform(df)\n",
    "        lda_map[month][name] = lda\n",
    "        gc.collect()\n",
    "        # Print the topics found by the LDA model\n",
    "        # print(\"Topics found via LDA:\")\n",
    "        print_topics(\n",
    "            lda_map[month][name], count_vectorizer_map[month][name], number_words\n",
    "        )\n",
    "        print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "\n",
    "for month, dicts in vectorized_data_map.items():\n",
    "    for name, df in dicts.items():\n",
    "        if not path.isdir(\"Output/lda_wordcloud_by_month/lda/\" + month):\n",
    "            os.mkdir(\"Output/lda_wordcloud_by_month/lda/\" + month)\n",
    "        LDAvis_data_filepath = 'Output/lda_wordcloud_by_month/lda/' + month + '/ldavis_{}_{}_{}_{}.pickle'.format(month,name,number_topics, number_words)\n",
    "        LDAvis_prepared = sklearn_lda.prepare(lda_map[month][name], df, count_vectorizer_map[month][name])\n",
    "        #with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        #    pickle.dump(LDAvis_prepared, f)\n",
    "        pyLDAvis.save_html(LDAvis_prepared,'Output/lda_wordcloud_by_month/lda/' + month + '/ldavis_{}_{}_{}_{}.html'.format(month,name,number_topics, number_words))\n",
    "    \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "#with open(LDAvis_data_filepath) as f:\n",
    "#    LDAvis_prepared = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ricerca",
   "language": "python",
   "name": "ricerca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
